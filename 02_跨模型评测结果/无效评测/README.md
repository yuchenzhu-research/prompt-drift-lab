# 无效评测说明（invalid_results）

本目录用于存放/记录未通过一致性校验的评测 JSON，以及对应的失效原因。
无效并不等于“模型回答失败”，而是指 **评测产物本身不满足可复现/可统计的结构约束**，
因此会被汇总脚本排除在 `valid_results` 之外，以避免污染统计结果。

---

## 本轮无效原因（基于 invalid_report.md）

本轮仅出现 1 类无效问题：

### 1) total_mismatch（总分不一致）
- 文件：`judge_gemini_bundle_chatgpt_v2.json`
- 具体条目：`q3 long explicit.pdf`
- 错误：JSON 中写的 `total=7`，但按五维分数求和应为 `8`（calc=8）

这类错误的含义非常明确：  
> 评委输出的五维分数（A–E）与其自带的 total 字段不一致，导致“同一份评测结果内部自相矛盾”。

**为什么会发生？（机制解释，不是推测）**
- 在我们的评测协议里，`total` 是五维 A–E 的求和结果。
- 但评委模型可能会：
  1) 手算/心算总分时出错；或
  2) 先写了五维，后又修改其中某一维但忘了同步 total；或
  3) 输出模板被截断/编辑导致字段不同步。
- 汇总脚本在 strict 校验下会把这类自相矛盾视为无效，避免错误传播到 CSV。

**如何修复（推荐流程）**
- 以五维 A–E 为准，重新计算 total，并写回 JSON。
- 修复后重新运行汇总脚本，即可进入 `valid_results` 并产出 CSV。

---

## 无效 ≠ 模型失败：为什么要严格区分？

- **模型失败**：输出不满足结构/内容要求（这属于实验现象，应计入评分，比如 A_structure=0）。
- **评测产物无效**：评测 JSON 自身不满足一致性/可解析（这属于“数据质量问题”，应剔除后再统计）。

我们把两者分开，是为了保证：
1) 统计表（CSV）可复现、可审计；
2) 结果分析不会混入“评测文件错误”带来的假信号。

---
