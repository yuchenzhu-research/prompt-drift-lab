# 00_Evaluation Protocol

## 0. Scope and Priority of This Directory

This directory freezes the evaluation rules and execution procedures of Prompt Drift Lab, ensuring that results are **reproducible, auditable, and comparable**.

**Priority Order (Critical)**

1) `EVAL_PROTOCOL_ZH.md`: The only executable evaluation specification (rubric definitions, JSON contract, and hard evidence constraints).
2) `JUDGE_PROMPT_ZH.md`: The fixed prompt used by the evaluator (judge). In case of conflict, the protocol takes precedence.
3) `01_validity_criteria_ZH.md` and `02_scoring_dimensions_ZH.md`: Explanatory and auxiliary documents; they MUST NOT modify or override the protocol.

**Terminology Alignment (Step 4 Notes)**

- Any terms appearing in this directory such as "baseline", "versions", "prompt versions", or other prompt-related fields MUST be aligned with the official naming defined in `01_experiment_design/` and `02_prompt_variants/`. If inconsistencies arise, the definitions in 01/02 take precedence and MUST be unified here.
- The evaluation process is **agnostic to A/B distinctions**: A/B labels are used only on the prompt-authoring side. The evaluator relies solely on sample **metadata** (e.g., version tags in filenames or fields such as `prompt_version`) for logging and grouping, and MUST NOT treat these labels as scoring priors.

> Note: Some historical artifacts in the repository may contain a `_v2` suffix in filenames. This is a legacy naming artifact and **does not indicate multiple active protocol versions**.
> The authoritative evaluation standard is defined exclusively by `EVAL_PROTOCOL_ZH.md` and `JUDGE_PROMPT_ZH.md` in this directory.

---

## 1. Evaluation Targets and Inputs

### 1.1 Evaluated Outputs (Generators)

- Three-section structured outputs generated by different models under the same prompt conditions, stored as PDF files.

### 1.2 Input Bundle (Minimum Evaluation Unit)

One bundle consists of **16 PDF files**, covering:

- questions: `Q3`, `Q4`
- versions: `baseline`, `long`, `weak`, `conflict`
- trigger_types: `implicit`, `explicit`

Terminology clarification:
- The `versions` field here refers to *prompt version labels*. If `02_prompt_variants/` standardizes this as `prompt_version`, `prompt_id`, or similar fields, `versions` should be treated as a legacy synonym and replaced accordingly.
- During evaluation, `versions` are used only for logging and grouping, and MUST NOT be interpreted as A/B indicators or strength priors.

**File Naming Convention**

- `q{3|4} {version} {implicit|explicit}.pdf`
- Example: `q3 baseline explicit.pdf`

---

## 2. Evaluation Artifacts

Each evaluation produces **one JSON file** (strict JSON only, containing only protocol-defined fields) for downstream aggregation.

### 2.1 Cross-Model Judging Artifacts

Recommended naming:

- `judge_{judge_model}_bundle_{generator_model}.json`

> If existing artifacts in the repository use filenames with a `_v2` suffix, they may be retained for backward compatibility, as long as their contents comply with this protocol.

### 2.2 Self-Judging Artifacts

Recommended naming:

- `self_judge_{model}.json`

### 2.3 Storage Locations

- Passed validity checks (valid): `.../valid_results/...`
- Failed validity checks (invalid): `.../invalid_results/...`

Invalid artifacts are excluded from primary statistics but retained for analyzing evaluator drift or protocol non-compliance.

---

## 3. Evaluation Methods

### 3.1 Primary Method: Cross-Model Judging

- A judge model scores the 16 PDFs generated by another model, file by file.
- Primary conclusions are derived from **valid cross-model judging results** only.

If a judge model frequently violates the protocol (e.g., produces non-strict JSON, missing fields, or evidence violations), its outputs are classified as invalid and analyzed separately, but excluded from primary statistics.

### 3.2 Supplementary Method: Self-Judging

- A generator model evaluates its own 16 PDFs under the same protocol.
- Purpose: consistency checks, bias diagnostics, and reference comparisons; self-judging results MUST NOT serve as the sole basis for primary conclusions.

---

## 4. Validity and Invalid Conditions

A judge output is classified as **invalid** if any of the following occur:

- Output is not strict JSON (e.g., contains Markdown, explanatory text, prefixes, or suffixes)
- JSON contract mismatch, missing fields, or incorrect item counts
- `total` score does not equal the sum of the five dimensions
- `A_structure == 0` while any of `B/C/D/E` is non-zero
- Evidence violates hard constraints (e.g., contains ellipses, is not a verbatim excerpt, or provides empty evidence with non-zero scores)
- `aggregates` are not recomputable from `per_file_scores`

---

## 5. Bias Control

- **Blind judging**: Evaluators MUST NOT be influenced by prompt role labels and MUST rely solely on PDF content and filenames.
- **A/B agnosticism**: Evaluators MUST NOT interpret sample labels as A/B comparisons or perform cross-sample comparisons; labels are used only for grouping and logging.
- **Unified protocol**: All evaluations MUST use the same `EVAL_PROTOCOL_ZH.md` and `JUDGE_PROMPT_ZH.md`.
- **Metadata logging (if available)**: Record model name/version, date, inference mode, temperature, top_p, etc., to support reproducibility diagnostics.
