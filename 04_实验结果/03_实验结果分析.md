# 04 实验结果分析（Experimental Results Analysis）

本节基于 `04_实验结果/cross_model_eval_results/valid_results/score_tables/` 中生成的
结构化统计结果，对 Prompt Drift 在不同模型、不同提示词版本与触发条件下的行为表现进行系统分析。
本节关注的是**可复现的行为模式与结构性差异**，而非对单一数值的绝对优劣判断。

---

## 4.1 结果数据与分析视角说明

### 4.1.1 主方法与辅助方法的角色区分
- **主方法（Main Method）**：跨模型互评（cross-model evaluation），作为主要证据来源  
- **辅助方法（Supporting Method）**：模型自评（self-evaluation），用于对照与偏差分析  

除非特别说明，本节所有结论均以**主方法结果**为依据，辅助方法仅用于解释模型自我感知与外部评价之间的差异。

---

### 4.1.2 分析维度
实验结果围绕以下四个核心维度展开：
1. **模型间差异**（generator-level behavior）
2. **显式 / 隐式触发差异**（explicit vs implicit）
3. **提示词版本扰动效应**（baseline / long / weak / conflict）
4. **评委一致性与分歧结构**（inter-judge agreement）

---

## 4.2 跨模型整体表现（Generator-level）

基于 `main_method_by_generator.csv`，不同模型在跨模型互评下呈现出稳定且可区分的行为模式：

- **ChatGPT** 在整体平均分、显式与隐式条件下均表现出较高稳定性；
- **Gemini** 在显式条件下接近 ChatGPT，但在隐式条件下出现明显性能下降；
- **Claude** 在多数隐式条件下分数接近下界，呈现出近似系统性失效的特征。

这一结果表明，**Prompt Drift 的主要影响并非体现在“谁的平均分更高”，而体现在模型对隐式结构约束的鲁棒性差异**。

---

## 4.3 显式与隐式触发条件的系统性差异

### 4.3.1 显式触发：模型能力差异的常规展开
在 explicit 条件下：
- 不同模型均能较好识别任务目标与输出结构；
- 分数差异主要体现在执行质量，而非结构性失败；
- Prompt 版本（baseline / long / weak / conflict）对结果有影响，但未引发系统性崩溃。

这说明在显式指令条件下，Prompt Drift 更像是**性能扰动**，而非行为失控。

---

### 4.3.2 隐式触发：结构性失效的集中出现
在 implicit 条件下，情况发生本质变化：
- 多个模型出现 **A_structure = 0** 的高频现象；
- 一旦结构维度失效，其余维度（B–E）随之归零；
- 不同模型在隐式条件下呈现出明显分层，而非连续退化。

这表明，**隐式提示并非“更弱的显式提示”，而是触发了模型在指令理解层面的不同工作机制**。

---

## 4.4 Prompt 版本扰动的影响模式

基于 `main_method_by_version.csv` 与 `main_method_by_question_version.csv`：

- **baseline / long** 版本在显式条件下差异有限；
- **weak / conflict** 版本在隐式条件下显著放大失效概率；
- 提示词“变长”本身并非主要风险源，**语义冲突与约束弱化才是 Drift 的关键诱因**。

这一结果否定了“prompt 越长越容易出错”的简单直觉，
并指向 **语义一致性与约束清晰度** 才是影响模型稳定性的核心因素。

---

## 4.5 题目层面的稳定性与泛化性

从 `main_method_by_question.csv` 可观察到：
- Q3 与 Q4 在总体趋势上保持一致；
- Prompt Drift 的核心现象并不依赖于具体题目内容；
- 题目主要影响的是失效出现的“具体位置”，而非是否发生失效。

因此，本实验捕捉到的行为模式具有一定的**跨任务稳定性**。

---

## 4.6 评委一致性分析：分歧不是噪声

基于 `main_method_inter_judge_agreement.csv`：

- 在显式条件下，不同评委之间的评分高度一致；
- 在隐式条件下，评委分歧显著增加，尤其集中在 **A_structure** 维度；
- 分歧往往对应“是否判定为结构失败”的临界样本，而非随机波动。

这说明评委分歧本身是一种**有信息量的信号**，
反映了模型输出在隐式条件下处于“结构可接受 / 不可接受”的模糊边界。

---

## 4.7 辅助方法：自评与互评的系统性偏差

从 `supporting_method_by_generator.csv` 与对应分表可见：
- 模型自评普遍高于跨模型互评；
- 自评在隐式条件下对结构失败的敏感度显著低于外部评委；
- 自评更容易忽略格式偏移、隐性违规等问题。

这进一步验证了**仅依赖 self-evaluation 会系统性高估模型在 Prompt Drift 条件下的鲁棒性**，
也从方法论上支持采用跨模型互评作为主评测手段。

---

## 4.8 小结：从“性能差异”到“行为漂移”

综合以上分析，可以得出以下关键认识：

1. Prompt Drift 并非连续退化，而是呈现出**触发式的结构崩溃**；
2. 隐式提示是最稳定、最强的失效触发条件；
3. 不同模型在 Drift 面前表现出**质的差异**，而非单纯性能高低；
4. 评测分歧本身是理解模型边界的重要线索，而非应被简单消除的噪声。

这些结果表明，Prompt Drift 是一种值得被单独研究的模型行为现象，
其影响超出了传统“prompt engineering 技巧优化”的范畴，
并与模型的指令理解、结构约束与对齐机制密切相关。
