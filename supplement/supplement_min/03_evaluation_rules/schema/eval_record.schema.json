{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://prompt-drift-lab/schema/eval_record.json",
  "title": "Prompt Drift Lab - Eval Record",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "eval_id",
    "run_id",
    "created_at",
    "protocol_version",
    "judge_model",
    "generator_model",
    "question_id",
    "prompt_version",
    "length_variant",
    "instruction_variant",
    "metrics"
  ],
  "properties": {
    "eval_id": {
      "type": "string",
      "description": "Unique identifier for this evaluation record (timestamp or hash)."
    },
    "run_id": {
      "type": "string",
      "description": "Identifier of the generation run being evaluated."
    },
    "created_at": {
      "type": "string",
      "format": "date-time",
      "description": "Evaluation creation time in ISO 8601."
    },
    "protocol_version": {
      "type": "string",
      "description": "Version of the evaluation protocol (e.g., EVAL_PROTOCOL_v2)."
    },
    "judge_prompt_version": {
      "type": "string",
      "description": "Optional: version tag/name of the judge prompt."
    },
    "judge_model": {
      "type": "string",
      "description": "Model used as judge (e.g., gemini/chatgpt/claude)."
    },
    "generator_model": {
      "type": "string",
      "description": "Model whose output is being evaluated."
    },
    "question_id": {
      "type": "string",
      "pattern": "^q[1-9][0-9]*$",
      "description": "Question identifier (q1, q2, ...)."
    },
    "prompt_version": {
      "type": "string",
      "enum": [
        "prompt_a",
        "prompt_b"
      ],
      "description": "Prompt version used during generation."
    },
    "length_variant": {
      "type": "string",
      "enum": [
        "short",
        "long"
      ],
      "description": "Length variant of the prompt setting."
    },
    "instruction_variant": {
      "type": "string",
      "enum": [
        "explicit",
        "implicit"
      ],
      "description": "Instruction style variant."
    },
    "source_artifact": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "type",
        "path"
      ],
      "properties": {
        "type": {
          "type": "string",
          "enum": [
            "pdf",
            "md",
            "txt",
            "json"
          ],
          "description": "Artifact type of the generator output."
        },
        "path": {
          "type": "string",
          "description": "Repo-relative or filesystem path to the artifact."
        },
        "sha256": {
          "type": "string",
          "description": "Optional checksum for integrity."
        }
      },
      "description": "Pointer to the evaluated artifact (e.g., a PDF)."
    },
    "metrics": {
      "type": "object",
      "description": "Metric scores + evidence. Keys should follow metric_key in docs/terminology.md.",
      "additionalProperties": {
        "$ref": "#/definitions/metric_result"
      }
    },
    "overall": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "score": {
          "type": "number",
          "minimum": 0
        },
        "max_score": {
          "type": "number",
          "minimum": 0
        },
        "summary": {
          "type": "string"
        }
      },
      "description": "Optional overall score/summary."
    },
    "evidence_snippets": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/evidence_snippet"
      },
      "description": "Optional: extracted snippets supporting key judgments."
    },
    "notes": {
      "type": "string",
      "description": "Optional free-form notes."
    }
  },
  "definitions": {
    "metric_result": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "score"
      ],
      "properties": {
        "score": {
          "type": "number",
          "description": "Metric score (scale defined by your rubric)."
        },
        "max_score": {
          "type": "number",
          "description": "Optional: maximum score for this metric."
        },
        "label": {
          "type": "string",
          "description": "Optional short label, e.g., PASS/FAIL or A/B/C."
        },
        "rationale": {
          "type": "string",
          "description": "Short reasoning for the score."
        },
        "evidence": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/evidence_snippet"
          },
          "description": "Evidence snippets supporting this metric."
        }
      }
    },
    "evidence_snippet": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "text"
      ],
      "properties": {
        "text": {
          "type": "string"
        },
        "location": {
          "type": "string",
          "description": "Optional pointer: page/section/line range, if available."
        }
      }
    }
  }
}