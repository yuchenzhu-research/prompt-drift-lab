# supplement/04_results/03_processed_evaluations — Processed evaluations

This directory stores **deterministic, derived artifacts** produced from preserved judge outputs.

- Source inputs: `/supplement/04_results/02_raw_judge_evaluations/`
- No PDFs are parsed during processing.
- No text is reinterpreted; processing operates only on judge JSON bundles and recorded metadata.

---

## Layout

Each judge run is written to its own subdirectory:

- `/supplement/04_results/03_processed_evaluations/<judge_version>/`

Within a `<judge_version>` directory:

- Per-record audit trail:
  - `valid_evaluations/**/record_*.json`
- Derived summary tables (paper-cites):
  - `summary_tables/scores_long.csv`
  - `summary_tables/scores_grouped.csv`
  - `summary_tables/excluded_records.jsonl`
  - `summary_tables/run_meta.json`

The CSV files in `summary_tables/` are the primary inputs for reporting and analysis.

---

## Provenance and regeneration rules

- Every `record_*.json` is generated from raw judge bundles under:
  - `/supplement/04_results/02_raw_judge_evaluations/`
- Each record corresponds to one evaluated file and preserves the evaluated file name exactly.
- Records and tables are generated by scripts in `/supplement/tools/`.

Records are regenerated **only when input artifacts or validity conditions change**.  
Previously validated records are preserved and are not overwritten.

---

## What this directory does not contain

- Evaluation rules and validity definitions  
  → `/supplement/03_evaluation_rules/`
- Raw model outputs (PDFs)  
  → `/supplement/04_results/01_raw_model_outputs/`
- Raw judge bundles  
  → `/supplement/04_results/02_raw_judge_evaluations/`