# 无效评测（Invalid Evaluations）

本目录用于**集中记录并隔离**在评测过程中出现的“不可用/不可信”评测结果。
这些结果的共同特征是：**无法作为定量统计依据**，否则会对整体结论造成系统性偏差。

---

## 1. 判定为“无效评测”的标准（满足其一即判无效）

为保证可复现与可审计，本项目将以下情况定义为无效评测：

1. **协议未遵循（PROTOCOL_VIOLATION）**
   - 评测模型未按《使用的评测协议.md》执行（例如：跳过必答项、改变评分尺度、追加/删减维度、输出与协议要求不一致）。

2. **输出不可解析（UNPARSABLE_OUTPUT）**
   - 输出结构或格式崩坏，导致无法稳定提取分数/标签/维度（例如：缺少关键字段、混入大量无关内容、格式不闭合、结构被打断）。

3. **评测对象不完整（INCOMPLETE_COVERAGE）**
   - 未覆盖应评测的样本集合（例如：只评了部分文件、遗漏某些模型/版本/问题，或样本对应关系不清导致无法对齐）。

4. **拒评/回避（JUDGE_REFUSAL_OR_EVASION）**
   - 明确拒绝评测、回避给分，或以安全/政策为由输出与评测无关的内容，从而无法得到可用评分。

5. **自相矛盾/不可重复（INTERNAL_INCONSISTENCY）**
   - 同一输出内部出现不可调和的矛盾（例如：同一维度给出冲突分数/结论），且无法通过最小清洗恢复。

> 备注：本目录强调“**判定标准**”而非“失败原因解释”。失败原因的机制讨论会在 `04_实验结果/03_实验结果分析.md` 中统一完成。

---

## 2. 目录结构与角色定位

本目录按评测方法区分为两类：

### 2.1 主方法：跨模型互评（Primary: Cross-model Judging）
路径：`主方法_跨模型互评/`

定义：由三个模型相互担任评测者，对另外两个模型的输出进行评测（例如：A 评 B/C，B 评 A/C，C 评 A/B）。

为什么是“主方法”：
- 更接近真实使用场景（外部评审/第三方评测）。
- 在统计上更能反映“评测者-被评测者”组合下的稳定性与漂移敏感性。

### 2.2 辅助方法：模型自评（Auxiliary: Self-judging）
路径：`辅助方法_模型自评/`

定义：模型对自身输出进行评测（self-judge）。

为什么只能作为“辅助”：
- 可能存在系统性偏差（自我一致性/自我辩护倾向），不宜作为主统计依据。
- 主要用于：补充对照、排查协议执行问题、提供额外失败模式线索。

---

## 3. 无效评测的使用规则（非常重要）

- **不纳入任何定量统计与主结论**。
- 仅用于：
  1) 记录评测失效案例；
  2) 归纳失败模式（failure modes）；
  3) 反向约束评测协议与提示词的可执行性边界。

> 简而言之：这里的文件回答“**哪些评测不可信**”，而不是“这些评测为什么会失败”。

---

## 4. 文件级标注规范（建议执行）

为便于追踪与复核，建议每个无效评测文件在开头或元信息处包含一个“失效标签”（可多选）：

- `PROTOCOL_VIOLATION`：未遵循评测协议
- `UNPARSABLE_OUTPUT`：输出不可解析
- `INCOMPLETE_COVERAGE`：样本覆盖不完整
- `JUDGE_REFUSAL_OR_EVASION`：拒评/回避
- `INTERNAL_INCONSISTENCY`：内部自相矛盾

同时建议补充最小必要信息（1 行即可）：
- `judge_model`（评测者模型）
- `target_model`（被评测者模型）
- `question_id / prompt_variant`（对应问题与提示词版本）
- `run_id / timestamp`（如有）

---

## 5. 与协议文件的关系

- 《使用的评测协议.md》：规定“应该如何评测”。
- 《使用的提示词清单.md》：规定“使用了哪些评测/生成提示词”。
- 本目录（无效评测）：记录“哪些评测没有达到可用标准，因此被排除”。

---

## 6. 快速检查清单（用于人工复核）

当你不确定某条评测是否应归入无效评测时，可按以下顺序快速判断：

1. 是否按协议给出了**完整维度**与**可提取的结论/分数**？
2. 是否覆盖了应评测的**完整样本集合**，且对应关系清楚？
3. 是否存在拒评/回避导致无法得到有效评分？
4. 输出是否自洽，是否能在不“改内容”的前提下被解析？

满足任一条失败，即归入本目录。

