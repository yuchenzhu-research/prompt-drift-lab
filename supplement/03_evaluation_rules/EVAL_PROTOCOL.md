# 00_Evaluation Protocol

## 0. Scope and Priority of This Directory

This directory **fixes** the evaluation rules and execution procedures of Prompt Drift Lab. The goal is to ensure that all evaluation results are **reproducible, auditable, and directly comparable**.

### Priority Order

1) `EVAL_PROTOCOL_ZH.md`: The executable evaluation specification, including rubric definitions, JSON contracts, and hard evidence constraints.
2) `JUDGE_PROMPT_ZH.md`: The fixed prompt used by the evaluator (judge). It implements the protocol and does not introduce additional rules.
3) `01_validity_criteria_ZH.md` and `02_scoring_dimensions_ZH.md`: Explanatory materials that clarify rule semantics and scoring dimensions. These documents do not modify or override the protocol.

All filenames, field names, and identifiers in this directory use English and are consistent across the project.

---

## 1. Evaluation Targets and Inputs

### 1.1 Evaluated Outputs

- Three-section structured outputs generated by different models under identical prompt conditions, stored as PDF files.

### 1.2 Input Bundle

Each evaluation bundle contains **16 PDF files**, covering the following combinations:

- `question`: `Q3`, `Q4`
- `prompt_variant`: `baseline`, `long`, `weak`, `conflict`
- `trigger_type`: `implicit`, `explicit`

During evaluation, prompt variants are used only as **recorded metadata for grouping**. They are not treated as A/B indicators or scoring priors.

### File Naming Convention

- `q{3|4} {prompt_variant} {implicit|explicit}.pdf`
- Example: `q3 baseline explicit.pdf`

---

## 2. Evaluation Artifacts

Each evaluation produces **one strict JSON file** containing only protocol-defined fields. This artifact is used for aggregation and verification.

### 2.1 Cross-Model Judging Artifacts

- `judge_{judge_model}_bundle_{generator_model}.json`

### 2.2 Self-Judging Artifacts

- `self_judge_{model}.json`

### 2.3 Storage Locations

- Valid evaluations: `supplement/04_results/02_cross_model_evaluation/valid_evaluations/`
- Invalid evaluations: `supplement/04_results/02_cross_model_evaluation/invalid_evaluations/`

Invalid artifacts are excluded from primary statistics but retained for protocol compliance analysis.

---

## 3. Evaluation Methods

### 3.1 Primary Method: Cross-Model Judging

- A judge model scores the 16 PDFs generated by another model, file by file.
- Primary conclusions are derived exclusively from **valid cross-model judging records**.

Judge outputs that violate the protocol (e.g., non-strict JSON, missing fields, or evidence violations) are classified as invalid and excluded from primary statistics.

### 3.2 Supplementary Method: Self-Judging

- A generator model evaluates its own 16 PDFs using the same protocol.
- Self-judging records are used for consistency checks and bias diagnostics and do not serve as the sole basis for primary conclusions.

---

## 4. Validity and Invalid Conditions

A judge output is classified as **invalid** if any of the following conditions occur:

- The output is not strict JSON (e.g., contains Markdown, explanatory text, prefixes, or suffixes)
- The JSON contract is violated (missing fields or incorrect item counts)
- The `total` score does not equal the sum of the five scoring dimensions
- `A_structure == 0` while any of `B/C/D/E` is non-zero
- Evidence violates hard constraints (e.g., ellipses, non-verbatim excerpts, or empty evidence with non-zero scores)
- `aggregates` cannot be recomputed from `per_file_scores`

---

## 5. Bias Control and Recording Requirements

- **Blind judging**: Evaluators rely solely on PDF content and filename metadata. Prompt roles and labels do not influence scoring.
- **A/B agnosticism**: Sample labels are used only for grouping and logging, not for cross-sample comparison or relative strength judgment.
- **Unified protocol**: All evaluations use the same `EVAL_PROTOCOL_ZH.md` and `JUDGE_PROMPT_ZH.md`.
- **Metadata recording**: Model identifier, execution date, inference mode, and sampling parameters (e.g., `temperature`, `top_p`) are recorded when available.
