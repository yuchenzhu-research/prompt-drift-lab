# Evaluation Protocol â€” Specification

## 0. Scope and Role of This Document

This document defines the **evaluation workflow and rule composition** used in Prompt Drift Lab.
It specifies how evaluation artifacts are produced, validated, and interpreted.

This document **does not define the JSON output contract**.
The JSON structure and field requirements are defined exclusively by:

- `schema/eval_record.schema.json`

This protocol **consumes schema-valid evaluation records** and must be read together with the corresponding judge prompt.

---

## 1. Evaluation Targets and Inputs

### 1.1 Evaluated Outputs

- Structured model outputs generated under controlled prompt conditions.
- Outputs may be stored as PDF or other static artifact formats.

### 1.2 Input Bundle

Each evaluation bundle contains a fixed set of artifacts defined by the experimental design.
All bundle composition details are recorded as metadata and are **not** used as scoring priors.

---

## 2. Evaluation Artifacts

Each evaluation produces **one strict JSON file** that **must conform to** the schema defined in:

- `schema/eval_record.schema.json`

This JSON artifact is the sole input for aggregation and verification.

---

## 3. Evaluation Methods

### 3.1 Primary Method: Cross-Model Judging

- A judge model evaluates artifacts generated by another model.
- Each artifact is evaluated independently.
- Primary conclusions are derived exclusively from **schema-valid cross-model evaluation records**.

Judge outputs that violate the JSON schema or evidence constraints are classified as **invalid** and excluded from primary statistics.

### 3.2 Supplementary Method: Self-Judging

- A generator model may evaluate its own outputs using the same protocol.
- Self-judging records are used for consistency checks only and do **not** serve as the sole basis for conclusions.

---

## 4. Validity and Invalid Conditions

A judge output is classified as **invalid** if any of the following conditions occur:

- The output is not strict JSON.
- The output violates the JSON schema (missing or malformed fields).
- Required evidence fields are missing or non-verbatim.
- Structural or logical constraints defined by the evaluation rules are violated.

Invalid records are excluded from primary aggregation but retained for protocol compliance analysis.

---

## 5. Bias Control and Recording Requirements

- **Blind judging**:
  Evaluators rely solely on artifact content and recorded metadata.

- **A/B agnosticism**:
  Metadata labels are used only for grouping and logging, not for relative strength judgments.

- **Unified protocol**:
  All evaluations use this protocol together with the corresponding judge prompt and JSON schema.