# 实验设计总览
01_实验设计（Experiment Design）

本目录定义 Prompt Drift Lab 的实验设计与评测协议。
目标不是比较“模型答得对不对”，而是系统性观察：

- 在结构化输出任务中
- 当提示词发生微小变化（长度、措辞、约束强度、冲突）
- 大语言模型是否出现：
  1) 指令遵循率下降
  2) 结构/格式崩坏
  3) 语义漂移（答非所问 / 越界 / 改任务）

本实验强调：
【是否按要求做事】，而非【内容是否正确】。


==================================================
1. 核心评测单元：Eval Bundle
==================================================

本项目的最小可复现实验单元称为 Eval Bundle。

一个 Eval Bundle = 
- 一组固定实验条件下生成的输出文件（通常为 PDF）
- 对每个输出文件的 rubric 打分
- 对不可评测样本的 invalid 归档说明

Eval Bundle 的作用是：
- 把“零散 PDF 输出”升级为“可统计、可复现、可复盘”的实验批次


--------------------------------------------------
1.1 Eval Bundle 示例规模
--------------------------------------------------

典型例子：

- Questions：Q3, Q4
- Prompt Versions：baseline / long / weak / conflict
- Trigger Types：implicit / explicit

总样本数：
2 × 4 × 2 = 16 个输出文件

（可扩展加入：模型维度、重复次数）


--------------------------------------------------
1.2 Eval Bundle 结果文件（JSON）
--------------------------------------------------

每个 Eval Bundle 最终应汇总为一个 JSON 文件，例如：

- eval_bundle_q3q4_16.json

该文件至少包含三部分：

1) bundle_meta：实验条件说明
2) per_file_scores：每个文件的 rubric 打分
3) invalid_files：不可评分样本说明


==================================================
2. 实验因素与变量（Factors）
==================================================

本项目默认将“一次模型输出”定义为以下多维组合（可扩展）：


--------------------------------------------------
2.1 Question（题目）
--------------------------------------------------

取值：
- Q1, Q2, Q3, Q4

说明：
- 每个 Q 代表一类结构化输出任务
- 建议保持任务类型多样，例如：
  - 严格模板输出
  - 多段结构（三段式）
  - 冲突约束
  - 推理 / 三段论结构

目的：
- 避免结论只对某一种任务形式成立


--------------------------------------------------
2.2 Prompt Version（提示词版本）
--------------------------------------------------

取值：
- baseline
- long
- weak
- conflict

定义：

baseline：
- 你认为最“正常 / 合理”的提示词版本
- 作为对照基线

long：
- 更长
- 更多解释
- 更多重复强调约束
- 用于检验：
  “提示词越长，模型是否真的越听话？”

weak：
- 弱化硬性约束
- 减少 must / strict / 禁止类语言
- 用于检验：
  结构是否依赖“强指令”才能维持

conflict：
- 引入冲突或歧义指令
- 例如同时出现“必须三段式”与“自由发挥”
- 用于观察：
  模型如何选择、妥协或直接崩坏


--------------------------------------------------
2.3 Trigger Type（触发强度）
--------------------------------------------------

取值：
- implicit
- explicit

implicit：
- 结构要求偏“暗示 / 推荐”
- 例如：
  “建议使用三段结构…”

explicit：
- 结构要求偏“硬性 / 必须 / 严格遵守”
- 例如：
  “必须严格按照以下三段式输出，否则视为失败”
  “strict compliance required”

目的：
- 研究结构约束强度对遵循率的影响


--------------------------------------------------
2.4 Generator Model（被测模型）
--------------------------------------------------

示例取值：
- chatgpt
- gemini
- claude
- …

建议：
- 记录具体型号或 UI 显示版本（如果可见）
- 不同模型之间不做能力优劣判断
- 只比较其指令遵循与漂移行为模式


--------------------------------------------------
2.5 Replicate（重复次数，可选）
--------------------------------------------------

取值：
- r1, r2, r3, …

适用场景：
- 温度 > 0
- 或同一条件下多次生成存在明显随机性

目的：
- 区分“偶然失误”与“系统性漂移”


==================================================
3. 输出文件与命名规范
==================================================

--------------------------------------------------
3.1 输出文件格式
--------------------------------------------------

建议：
- 每次模型输出保存为 PDF
- 不进行任何人工修饰或二次编辑
- 原样保留模型输出内容

推荐在 PDF 首页或顶部注明：
- Generator Model
- 生成时间
- Prompt Version
- Trigger Type
- Question ID
- （如有）temperature / top_p 等参数


--------------------------------------------------
3.2 文件命名规范（强烈建议）
--------------------------------------------------

统一命名格式：

q{QID}_{version}_{trigger}_{model}_r{rep}.pdf

示例：
- q3_baseline_explicit_chatgpt_r1.pdf
- q4_conflict_implicit_gemini_r2.pdf

说明：
- 使用下划线而非空格
- 便于后续脚本处理与人工核查


==================================================
4. 评测协议与 Rubric 对接
==================================================

--------------------------------------------------
4.1 Rubric 位置
--------------------------------------------------

具体评分标准定义在：

03_评测规则/

本目录只负责说明：
- 如何设计实验
- 如何生成样本
- 如何组织评测数据


--------------------------------------------------
4.2 invalid 样本的定义（非常重要）
--------------------------------------------------

invalid 用于区分“不可评分”与“评分很低”。

属于 invalid 的情况包括：
- 模型拒绝执行任务
- 完全变成闲聊或改写任务
- 输出严重截断，无法判断结构
- 完全无关内容

不属于 invalid 的情况：
- 结构存在但做得很差
- 明显漂移但仍可评分
- 违反部分约束

原则：
- 能打分的，就打低分
- 不能打分的，才进 invalid


==================================================
5. 最小可复现实验流程（当前版本）
==================================================

在不依赖任何工程脚本的情况下，你现在可以这样跑：

1) 固定一组 Question × Prompt Version × Trigger Type
2) 用指定模型生成输出，保存为 PDF
3) 按命名规范整理文件
4) 根据 Rubric 对每个文件人工评分
5) 汇总为一个 eval_bundle_*.json
6) 明确标注 invalid 样本与原因

