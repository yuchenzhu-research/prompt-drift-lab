# 威胁与局限（Threats to Validity & Limitations）

本节讨论 Prompt Drift Lab 在当前阶段的主要威胁与局限，用于界定结论适用范围，并为后续扩展实验提供路线。

---

## 1. 构念效度（Construct Validity）

### 1.1 “漂移”定义的边界
本项目将 prompt drift 主要操作化为三类可观测现象：**指令遵循率下降、结构/格式崩坏、语义漂移（改任务/越界/跑题）**。但这些现象之间存在交叠：例如结构崩坏可能只是长度溢出导致的格式丢失，而非真正的任务理解漂移。当前做法通过 Rubric 维度进行区分，但仍可能出现分类模糊。

### 1.2 Rubric 维度与量表的主观性
Rubric 以 0–2 量表评估结构遵循、快照约束、可执行性、完整性与漂移失败等维度。该量表的离散性较强，且不同维度之间可能相关（如结构崩坏往往伴随完整性下降），导致某些错误被“重复计分”或被“掩蔽”。后续需通过更精细的定义与示例库减少歧义，并报告维度相关性与一致性。

### 1.3 invalid 规则对统计的影响
本项目将 invalid 严格限定为“不可评分样本”（拒答、空输出、严重截断或结构不可判定），而非“表现很差”。该策略有助于数据卫生，但 invalid 的边界仍可能在少数样本上引发判断分歧，从而影响 invalid rate 与总体分布。后续应在 rubric 中加入更明确的 invalid 判定示例。

---

## 2. 内部效度（Internal Validity）

### 2.1 干扰变量与“非提示词因素”
即使固定提示词版本，输出仍可能受以下因素影响：对话上下文是否彻底重置、UI 默认系统提示、模型服务端更新、限流导致的截断、生成策略参数不可见（例如 hidden system policies）。当前协议中要求 “new chat per sample” 并记录可见参数，但仍无法完全隔离所有系统级变化。

### 2.2 版本差异并非严格的“单因素扰动”
尽管实验意图是最小扰动，但提示词版本（baseline/long/weak/conflict）可能同时引入多个变化（长度、语气、约束强度、冲突程度）。若同时改变多个维度，会削弱因果归因能力。后续可引入更细粒度的消融（例如只改一句话、只改一个关键词）来增强解释性。

### 2.3 评分者偏差与回填偏差
若评分由单一评分者（或单一 judge model）完成，可能存在系统偏差：评分者对某类格式更宽容或更严格；评分时已知条件（版本/模型）也可能影响判断。后续应考虑：
- 评分时隐藏版本/模型信息（blind scoring）
- 双评分者与仲裁机制
- 或在同一 rubric 下做 judge model 的交叉评测以估计偏差

---

## 3. 外部效度（External Validity）

### 3.1 题集规模与任务覆盖有限
当前题集规模较小（例如仅 4 个问题，且本批次可能只用 Q3/Q4），任务类型与领域覆盖有限。结论可能仅适用于“结构化输出/模板遵循”这一类任务，而不代表开放问答、代码生成、长文写作等场景。后续应扩展题集维度：
- 不同结构类型（表格、JSON、混合格式、层级模板）
- 不同任务类型（检索指令生成、规划、推理、约束冲突）
- 不同输入长度与噪声水平

### 3.2 模型与版本的可迁移性
结论可能依赖具体模型家族、具体版本与具体时间窗口。尤其是商业模型存在持续迭代，导致同一提示词在不同日期输出行为变化。当前协议鼓励记录 model_version 与 time_window，但对“跨时间一致性”仍缺乏系统验证。后续应：
- 在不同时间点重复相同 eval bundle（temporal replicate）
- 或使用可固定版本的 API/开源模型作为参照

### 3.3 语言与文化因素
若题集与提示词主要为中文（或主要为英文），结论可能对另一语言不完全成立。不同语言可能引入不同的格式偏好、指令强度感知与安全策略触发差异。后续可设计中英对照题集与提示词版本，评估语言迁移效应。

---

## 4. 统计结论效度（Conclusion Validity）

### 4.1 样本量与不确定性
当每个条件仅 1 次输出（replicate=1）时，无法估计方差，也难以区分随机波动与系统性漂移。若要进行更稳健的比较（例如 long vs baseline 的差异），应增加每个 cell 的重复次数（建议 N≥3），并报告均值与离散程度（如标准差/置信区间或分位数）。

### 4.2 汇总指标与多重比较风险
若在多个维度、多个问题、多个模型上同时比较，容易出现“看起来有差异但只是偶然”的风险。当前阶段以探索为主，但在形成更强结论前，应明确比较假设并控制多重比较（例如预注册比较对、或仅报告主要比较）。

---

## 5. 可复现性与可审计性（Reproducibility & Auditability）

### 5.1 运行记录与产物对齐
若输出文件命名不统一、缺少运行参数记录或缺少题集/ rubric 版本标签，会导致难以复现与审计。当前方案通过：
- protocol.yaml 作为单一真值源
- 统一文件命名
- eval bundle JSON 固化评分结果
来提升可复现性。后续建议补充：
- 对题集、rubric、协议文件记录 checksum（如 SHA-256）
- 保存原始 prompt 文本与生成环境信息（UI/API）

### 5.2 人工干预与后处理
若对模型输出进行人工修饰（例如手工补齐格式、去掉多余段落）会污染证据链，使评分失真。当前协议要求保留原始输出 PDF 作为证据，后续应进一步明确：任何清洗/截取必须保留原始版本并记录规则。

---

## 6. 后续缓解路线（Mitigations Roadmap）

为降低上述威胁，后续优先级建议为：

1) **扩展重复次数**：每个条件 N≥3，报告离散程度  
2) **盲评与一致性**：至少双评分者或 judge model 交叉，建立仲裁流程  
3) **更细粒度消融**：从“四版本”细化到“一句/一词级扰动”  
4) **扩展题集覆盖**：结构类型、任务类型、语言对照  
5) **跨时间复测**：固定同一 bundle 在不同日期复跑，评估版本漂移

---

## 结论适用范围（Current Scope）

当前阶段结论主要适用于：
- 结构化输出/模板遵循类任务
- 在有限题集与有限模型集合上的探索性观察
- 以 rubric 评分为核心的定性-定量混合评测

在扩大题集、增加重复次数、强化评分一致性之前，不应将结果泛化为“模型总体能力”或“某模型优于另一模型”的结论。
