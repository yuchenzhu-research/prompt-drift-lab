# Prompt Drift Lab  
> 一个面向结构化提示词的可复现评测与失败模式归因实验

---

## 一句话简介
**Prompt Drift Lab** 研究：当提示词的结构、措辞或约束发生微小变化时，大语言模型在结构化输出任务中出现的**结构遵循率下降、格式崩坏与指令越界**等现象（Prompt Drift），并将其工程化为一套**可评测、可归因、可复用**的实验框架。

---

## 项目在做什么（What）
本项目以“三段式提示词转译器”为研究对象，系统考察不同提示词版本（baseline / structured / 消融变体）在固定题集上的行为差异，重点关注：

- 是否严格遵循指定输出结构
- 是否出现指令越界或格式漂移
- 漂移是否呈现稳定、可重复的失败模式
- 如何用**最小提示词改动**提升稳定性

---

## 为什么值得看（Why）
与“答得对不对”不同，本项目关注的是**模型是否按要求做事**：

- 不依赖 gold answer，避免内容正确性争议  
- 使用**硬性合规规则（可程序化）+ 行为维度分析**  
- 将现象收敛为**可复用的失败模式 taxonomy**  
- 给出可直接粘贴的**最小改动清单**

适合关注：Prompt Engineering、LLM Alignment、LLM Evaluation、Human-in-the-loop 行为研究。

---

## 项目结构总览（How to Read）

```text
01_实验设计/        # 实验目标、流程、问题集、标准输出结构
02_提示词版本/      # 提示词版本与消融变体（含 manifest）
03_评测规则/        # v2 评测协议、硬性合规规则、行为评分维度
04_实验结果/        # Results → Analysis → Attribution → Discussion
05_总结与展望/      # 项目总结、局限性与未来扩展路线
工程脚本/           # 自动化评测与统计脚本
README.md           # 项目入口说明（本文件）
