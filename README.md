## Reproducibility

All summary tables reported in this work are **not stored as static artifacts** in the supplement. Instead, they are **reproduced on demand** from the released raw outputs using a single canonical script.

### Canonical Reproduction Path

The complete reproduction process is defined as follows:

1. **Inputs**

   - Raw model outputs stored under `runs/<RUN_ID>/raw_outputs/`
   - Judged evaluation results stored under `runs/<RUN_ID>/judged_scores/`

2. **Canonical Aggregator**

   - All aggregation and scoring logic is implemented in:
     ```
     03_evaluation_rules/compute_scores.py
     ```
   - This file is treated as part of the evaluation protocol and is the **only authoritative implementation** of summary aggregation.

3. **Reproduction Script**

   - Summary tables are generated by executing:
     ```
     bash tools/reproduce_summary.sh <RUN_DIR>
     ```
   - The script is a thin wrapper and does not implement any scoring logic itself.

4. **Outputs**

   - The reproduced summary table is written to:
     ```
     <RUN_DIR>/summary.csv
     ```

### Design Rationale

To avoid duplicated sources of truth and to ensure strict reproducibility, generated summary tables are not included as static files in the supplement. Instead, the supplement releases:

- The full evaluation protocol and scoring rules
- The canonical aggregation implementation
- All raw and judged intermediate artifacts required for reproduction

This design ensures that every reported summary can be regenerated from first principles using the released code and data, without reliance on precomputed results.

