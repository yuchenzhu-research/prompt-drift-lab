# Prompt Drift Lab

**大语言模型结构化提示词稳定性评测与归因实验**  
*Prompt Drift Lab: A Reproducible Evaluation Framework for Instruction-Following Stability in LLMs*

> 中文为主；模型、协议、Prompt 与脚本保留英文命名，以对齐学术与工业界通用实践。  
> 本仓库面向两类读者：  
> - **科研评审 / 导师**：关注可复现性、评测严谨性与失败归因  
> - **工业 Mentor / 用人方**：关注提示词评测方法、风险识别与迭代闭环

---

## 1. 现实动机与研究背景（Why this exists）

在真实应用中，大语言模型的输出常因**提示词的格式、措辞、长度或约束表达的微小变化**而出现显著退化，例如：

- 指令遵循率下降（instruction following drop）
- 结构或格式崩坏（format break）
- 语义漂移或答非所问（semantic drift / off-topic）
- “看似听话但关键约束被忽略”的隐性失败（silent constraint violation）

我们将这类**对提示词微扰高度敏感、并导致行为失稳的现象**统称为 **Prompt Drift**。

与此同时，近期公开研究与案例表明：  
在 **AI 辅助审稿、文档理解、自动摘要** 等场景中，输入文档本身可能携带**人类难以察觉、但模型可读的隐藏指令**（如白色文本、极小字号、遮蔽层、Unicode 控制字符等），从而影响模型判断与输出。

这揭示了一个更普遍的风险背景：

> **LLM 对“内容（data）”与“指令（instruction）”缺乏天然边界，  
> Prompt Drift 与间接提示注入（Indirect Prompt Injection）在真实系统中可能相互叠加并放大影响。**

Prompt Drift Lab 并不直接构建防御系统，而是首先回答一个更基础的问题：  
**模型在多大程度上会因“看似无害”的提示词变化而失稳？这种失稳能否被系统性观测、评测与归因？**

---

## 2. 项目目标与核心思路

本项目的目标是将“提示词稳定性”从经验问题，转化为**可复现、可比较、可归因的评测问题**，并形成工程化实验资产。

核心思路包括：

- 固定题集与多版本 Prompt 设计，控制变量观察行为变化
- 保留模型原始输出，避免“只看分数不看失败样本”
- 采用**跨模型互评**作为主评测方式，以降低单一评委偏差
- 使用结构化 Rubric 拆解“漂移”与“失败类型”
- 明确区分：  
  - **执行性协议 / Prompt**（英文，供模型与评测流程使用）  
  - **解释性文档 / 实验记录**（中文，供人类分析与复现）

---

## 3. 仓库结构总览（Repository Overview）

本仓库采用“**科研论文 → 工程化目录**”的组织方式，将方法、评测与结果映射为清晰、可复现的目录结构：

```text

├── 01_实验设计/            # 研究问题、实验协议与威胁分析
├── 02_提示词版本/          # 不同 Prompt 设计方案与清单
├── 03_评测规则/            # 评测协议、Rubric 与评分脚本
├── 04_实验结果/            # 模型输出、评测结果与分析
├── 05_总结与展望/          # 实验结论、局限与后续研究路线
└── README.md               # 项目总览（本文件）
```
## 4. 各目录说明（与实际文件一一对应）

### 01_实验设计

该目录对应论文式研究中的 Method / Experimental Setup，用于定义研究问题、实验流程、控制变量与可复现协议。

- README.md  
  实验设计总体说明，解释研究动机、实验目标与整体设计逻辑。

- 实验设计_五步法.md  
  将 Prompt Drift 实验拆解为五个可复现步骤（任务定义 → Prompt 构造 → 运行 → 评测 → 归因），明确每一步的控制变量。

- 实验协议.yaml  
  实验执行协议（模型、温度、采样方式、格式约束等），用于保证跨模型、跨运行的一致性。

- 标准输出结构.md  
  定义模型输出的结构化格式，用于检测格式崩坏、字段缺失与结构偏移。

- 问题集.jsonl  
  固定评测问题集，作为所有 Prompt 版本的统一输入基准。

- 威胁与局限.md  
  对应学术论文中的 Threats to Validity，讨论实验设计与评测方法的潜在偏差与局限性。


### 02_提示词版本

该目录用于存放不同 Prompt 设计方案，构成 Prompt Drift 的主要自变量。

- 00_baseline_prompt_A.txt  
  基线 Prompt，不引入复杂结构或强约束。

- 01_structured_prompt_B.txt  
  结构化 Prompt，引入角色、步骤与显式格式要求。

- 02_conflict_prompt.txt  
  含冲突或多重约束的 Prompt，用于诱发指令冲突。

- 03_long_prompt.txt  
  冗长 Prompt，用于测试长度与信息密度对遵循率的影响。

- 04_weak_prompt.txt  
  弱约束 Prompt，用于观察约束缺失下的行为退化。

- PROMPT_MANIFEST.md  
  Prompt 版本说明文档，解释各 Prompt 的设计动机与预期影响。


### 03_评测规则

该目录对应论文中的 Evaluation / Metrics / Judging Protocol，用于定义如何判定 Prompt 是否“失效”。

- 00_评测协议.md  
  总体评测流程说明，定义评测顺序与判定原则。

- 01_硬性合规判定规则.md  
  用于检测格式、字段、结构等硬性违规（如 JSON 破坏、字段缺失）。

- 02_行为表现评分维度.md  
  行为层面的评分维度（相关性、完整性、逻辑一致性等）。

- EVAL_PROTOCOL.md  
  跨模型通用的评测执行协议。

- JUDGE_PROMPT.md  
  用于模型互评或自评的评测 Prompt。

- compute_scores.py  
  评测结果聚合与统计脚本，用于生成最终统计表。


### 04_实验结果

该目录对应论文中的 Results / Analysis，用于存放模型输出与评测结果。

- 01_模型原始输出/  
  各模型在不同 Prompt 条件下的原始输出结果。

- 02_跨模型评测结果/  
  跨模型互评与辅助评测结果，包括有效评测与无效评测样本。

- 03_实验结果分析.md  
  对实验结果的系统分析，包括失败模式归纳与初步归因。


### 05_总结与展望

该目录对应论文中的 Conclusion & Outlook，用于总结研究发现并提出未来研究方向。

- README.md  
  总结 Prompt Drift 的核心发现、实验贡献与后续可扩展方向。
