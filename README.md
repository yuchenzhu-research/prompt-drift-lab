# Prompt Drift Lab

> 一个关于大语言模型中提示词漂移（Prompt Drift）**与**指令遵循失败（Instruction Following Failure）的最小可复现实验工作坊。

---

## 项目简介

本项目旨在记录和分析：  
当用户设计的结构化提示词（prompt）与模型内部的系统级指令或默认行为发生冲突时，大语言模型在**格式遵循、任务目标保持与指令转译**方面可能出现的失败现象。

项目采用工作坊（workshop-style）形式，聚焦于：
- 现象记录（observation）
- 最小可复现实验（minimal reproducible setup）
- 初步失败归因（failure attribution）

而非完整 benchmark 或工程系统。

---

## 研究动机

在实际使用中，我们观察到以下问题：

- 即使提示词明确要求“不要回答问题，而是生成提示词”，模型仍可能直接进入“回答问题”模式；
- 在不同提示词版本下，相同问题的输出在结构稳定性和信息密度上存在显著差异；
- 当提示词与系统/平台级指令冲突时，用户提示词可能“失效”。

这些现象通常被笼统地归因于“模型不听话”，但其背后可能涉及：
- 指令层级优先级（instruction hierarchy）
- 模型默认目标偏置
- 提示词鲁棒性不足
- 工具/联网模式对输出模板的影响

本项目尝试以**可复现的方式**对这些问题进行初步探索。

---

## 实验设置（v0）

### 提示词版本
- **Prompt A（原始版本）**：三段式结构化提示词（早期版本）
- **Prompt B（修改版本）**：在 Prompt A 基础上，经结构强化后的版本

### 测试题目
共 4 道测试题，覆盖不同类型：
1. 日常事实型问题（如天气）
2. 日常经验型问题（如摄影参数）
3. 概念解释型问题
4. 元问题（关于模型行为本身）

详见：`testset/测试题_v0.md`

---

## 输出与记录

- 所有模型输出以 PDF 形式保存，按提示词版本分别归档：
  - `outputs/prompt_A/`
  - `outputs/prompt_B/`
- 本仓库**不对输出进行修改或清洗**，以保留原始失败形态。

---

## 初步观察（摘要）

在当前实验中，已观察到以下现象（详见 `reports/初步观察记录_v0.md`）：

- “简略事实回答”部分在部分情况下退化为对问题本身的复述；
- 事实段信息量显著不足，与预期的“事实快照”不符；
- 生成的二级提示词（给 ChatGPT / Gemini）在可执行性上存在退化；
- 不同提示词版本在格式遵循稳定性上存在差异，但均未完全避免目标偏移。

这些观察结果**尚不构成结论**，仅作为后续分析与扩展实验的基础。

---

## 当前局限

- 测试样本规模较小；
- 评估以人工观察为主，尚未引入量化指标；
- 尚未系统分析系统提示词与用户提示词的冲突机制。

---

## 后续计划（Roadmap）

- 引入基础的鲁棒性评测指标（format compliance / information density 等）
- 设计人工标注（human-in-the-loop）的失败类型标签体系
- 探索将提示词漂移类比为“概念漂移（concept drift）”的可能性
- 对齐相关学术工作与工业界探索（如 instruction hierarchy、alignment）

---

## 项目状态

- 当前版本：**v0（观察与记录阶段）**
- 本仓库将持续迭代，欢迎讨论与交流。

---

## 作者说明

本项目由计算机科学本科生独立完成，作为对大语言模型行为与对齐问题的探索性研究。  
项目内容以真实实验与诚实记录为原则，避免过度推断或夸大结论。
