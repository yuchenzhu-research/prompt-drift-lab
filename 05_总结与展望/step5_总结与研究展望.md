# Step5｜总结与展望（v2）
> 本章对 Prompt Drift Lab 的实验结果进行收束：给出可追溯的核心结论、失败模式归因、提示词设计启示，并明确局限性与后续扩展路线。  
> 实验结果与证据定位主要在 `04_实验结果/`，评测规则在 `03_评测规则/`，提示词集合在 `02_提示词版本/`。

---

## 1. 项目一句话概括（What This Lab Is）
本项目研究：当提示词的结构、措辞或约束发生微小变化时，LLM 在结构化输出任务中会出现的**结构遵循率下降、格式崩坏、指令越界与语义漂移**等现象（Prompt Drift），并尝试将这些现象**工程化地评测与归因**。

---

## 2. 核心结论（Key Takeaways）
> 说明：以下结论以“结构/指令遵循行为”为中心，不评事实正确性或知识质量。

- **结论 1：结构化提示词并不必然更稳**  
  强结构约束通常能提高“可判定性/可对齐性”，但也可能带来更高的脆弱性：在特定题型（尤其机制型问题）或约束冲突场景中，结构更容易坍塌或出现越界段落（详见 `04_实验结果/01` 与 `02`）。

- **结论 2：漂移并非随机噪声，而是可重复的失败模式**  
  多数违规可以收敛成少量稳定的失败模式（failure taxonomy），例如结构坍塌、越界解释、来源污染、深挖降级、额外段落噪声等（见 `04_实验结果/03`）。

- **结论 3：“段落职责”是三段式任务里最关键的稳定锚点**  
  当第二段（外部信息检索指引）与第三段（机制研究指引）的职责不够“不可混淆”时，模型会出现段落同质化、不可执行或被挤压退化现象（见 `04_实验结果/02` 与 `03`）。

- **结论 4：机制型问题更易触发“默认写作模板”与越界行为**  
  Q3/Q4 类问题更容易诱发解释、总结、建议等对话体裁倾向，从而导致额外段落与越界解释的连锁违规（见 `04_实验结果/02` 与 `03`）。

---

## 3. 失败模式归因（Failure Taxonomy & Attribution）
本项目将输出不稳定性主要归纳为以下可复用失败模式（详见 `04_实验结果/03_失败模式归因分析.md`）：

- **FM1 结构坍塌**：三段式缺段/多段/顺序错乱，或段落合并拆分导致不可判定  
- **FM2 越界解释**：事实快照渗入解释原理、建议、延伸讨论  
- **FM3 来源污染**：第一段出现“来源/链接/参考”等字样（即使未提供链接）  
- **FM4 检索指令不可执行**：第二段更像总结而非可复制的检索型指令  
- **FM5 深挖指引降级**：第三段与第二段同质化或缺少研究计划/分歧/证据对照  
- **FM6 额外段落噪声**：开场白/结尾总结/免责声明/附加建议导致结构破坏  
- **FM7 跨题不一致**：同一提示词版本在不同题目上结构模板不稳定

这些模式的价值在于：它们既能作为评测报告的“语言”，也能直接转化为提示词改动目标（见下一节）。

---

## 4. 设计启示（Design Implications）
结合结果与失败模式，本项目得到以下“可操作”的提示词设计原则（详见 `04_实验结果/04_讨论与设计启示.md`）：

1) **段落职责必须不可混淆**  
   每一段都应有唯一职责，并明确禁止做什么，否则模型会用默认写作模板“填空”。

2) **对触发词做免疫（抗默认体裁）**  
   对“解释/建议/总结/免责声明”等常见越界行为给出显式禁止与回退策略，减少连锁漂移。

3) **把“来源/证据/可验证”与第一段严格隔离**  
   强调可验证并不等于在事实快照里提来源；把来源要求限定在第二段能显著降低来源污染。

4) **短而硬优于长而软**  
   冗长、解释性提示词更容易造成目标层级混乱，通常第三段会优先退化；硬约束用短句更稳。

5) **加入自检/回退策略**  
   一旦生成越界内容，要求模型立即删除多余文本，仅保留三段结构，有助于抑制结构坍塌与噪声扩张。

---

## 5. 复现要点（Repro Checklist）
为了保证他人能复现本实验，项目将关键资产固定为以下“四件套”：

- **提示词集合（自变量）**：`02_提示词版本/`（含 P0–P4 与 `PROMPT_MANIFEST.md`）  
- **问题集（固定不变）**：`01_实验设计/问题集.jsonl`  
- **输出模板（唯一合法结构）**：`01_实验设计/标准输出结构.md`  
- **评测规则（v2）**：`03_评测规则/01_硬性合规判定规则.md` 与 `03_评测规则/02_行为表现评分维度.md`  
- **原始证据**：`04_实验结果/raw_outputs/`（可抽查）  
- **结果与讨论**：`04_实验结果/01–04`（Results→Analysis→Attribution→Discussion）

---

## 6. 局限性与外推边界（v2）
- **测试集规模与覆盖范围有限**：当前仅使用 4 道固定题目（Q1–Q4）。尽管覆盖“事实/技能/机制”三类风格，但样本量仍偏小，结论更适合作为现象与机制假设的证据，而非统计显著性意义上的稳定结论。

- **模型覆盖范围有限（单模型）**：本实验主要在 **ChatGPT（Web UI）** 条件下完成，尚未纳入 **Gemini、Claude** 以及开源模型（如 Llama/Qwen 等）的系统对照。因此更准确的表述应为：  
  > “在 ChatGPT（Web UI）这一运行条件下，三段式结构化提示在小幅变体扰动时的结构遵循与指令漂移现象”。

- **平台环境不可控因素**：Web UI 环境存在系统提示、内容策略、默认温度/采样等不可控因素（已在 `protocol.yaml` 中声明并视为常量处理），可能影响指令优先级与输出体裁倾向，从而影响跨时间稳定性。

- **评测方法的潜在偏差**：硬性合规规则可较稳定复核；但软性维度与自动化评审（LLM-as-a-judge）可能受到长度偏差、位置偏差等影响，因此本项目强调“结构/指令层面的可观察漂移”，避免将评分误读为“内容质量优劣”。

- **任务形态的外推限制**：本项目聚焦“三段式提示词转译器”这一结构化输出任务，结论不应直接外推到开放式问答、代码生成等任务形态；其价值更在于提供可复用的“结构遵循评测 + 失败模式归因”框架。

---

## 7. 后续扩展路线（v2）
- **扩展测试集与题型覆盖**：将题目从 4 道扩展到更系统的集合（例如 30–50 道），按类型分层（事实/技能/机制/对抗式指令/多约束冲突），并固定版本号（如 `eval_set.v2`），作为回归测试基准。

- **多模型对照矩阵**：在相同协议与评测规则下，引入多模型对照（ChatGPT / Gemini / Claude / 典型开源模型），形成“Prompt 变体 × 模型”的漂移对比矩阵，验证失败模式是否具有跨模型稳定性。

- **API 控制变量复现**：补充 API 运行（可控 temperature、seed、top_p 等），用于区分“提示词漂移”与“平台环境漂移”，并提供更强可复现性证据。

- **评审去偏与多评审一致性**：对软性维度引入长度控制、盲评策略，必要时采用多评审（judge ensemble）或人类抽检，以降低单一评审偏差带来的误差。

- **自动化回归与版本化**：将每次提示词改动纳入回归流程（固定题集、自动统计合规率/失败模式分布），输出趋势图与变更日志，形成“可迭代的 Prompt Drift Eval 套件”。

---

## 8. 本项目的可复用贡献（What Can Be Reused）
- 一套面向结构化提示任务的 **v2 硬性合规判定规则**（可程序化/可复核）
- 一套将现象收敛为可复用语言的 **失败模式 taxonomy**（可对照/可扩展）
- 一条从结果 → 分析 → 归因 → 讨论 → 最小改动的 **工程化研究叙事路径**
